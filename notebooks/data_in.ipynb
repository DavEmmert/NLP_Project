{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f62f80",
   "metadata": {},
   "source": [
    "# NLP Project Sectorlense Vertragschecker\n",
    "\n",
    "**Projectdescription**\n",
    "\n",
    "Reviewing software contracts is often a complex and error-prone task, particularly when\n",
    "assessing standardized requirements and identifying potential risks. Manual contract review\n",
    "can be time-consuming, leading to inconsistencies and oversight. To address this challenge,\n",
    "the project aims to develop an LLM-based contract checker that automates the review\n",
    "process. By leveraging predefined checklists and legal standards, the system will\n",
    "systematically analyze contracts, ensuring that required clauses are present while also\n",
    "detecting critical or unusual formulations. This will streamline contract evaluation and\n",
    "facilitate structured risk assessment, reducing both time and effort for legal professionals\n",
    "and businesses.\n",
    "\n",
    "The contract checker will incorporate three primary functionalities. A standard compliance\n",
    "check will verify whether contracts include the necessary clauses and if they adhere to\n",
    "established legal and business standards. Assessment based on standardized criteria will\n",
    "evaluate key contractual aspects to ensure completeness and compliance. Risk identification\n",
    "will highlight non-standard, ambiguous, or high-risk clauses, enabling users to assess their\n",
    "appropriateness compared to standard contract terms. Additionally, an optional risk\n",
    "detection feature could be introduced to flag further potential risks that may not be explicitly\n",
    "covered in the predefined checklist.\n",
    "\n",
    "The final deliverable will be a web application that enables users to upload contract\n",
    "documents and receive an automated structured review including insights on compliance\n",
    "and risk factors. This application will provide detailed feedback, highlight critical sections,\n",
    "and suggest improvements, making contract review more efficient and reliable.\n",
    "Development will build upon an existing prototype that includes both a frontend and basic\n",
    "functionality, allowing for enhancements in accuracy, usability, and scalability.\n",
    "\n",
    "**Meilensteine**:\n",
    "\n",
    "Milestone 1: Understanding existing prototype and defining key requirements (Week 1-2)\n",
    "\n",
    "Milestone 2: Developing/improving NLP-based contract analysis model (Week 3-6)\n",
    "\n",
    "Milestone 3: Integration into the web application (Week 7-8)\n",
    "\n",
    "Milestone 4: Testing and evaluation with real-world contracts (Week 9-10)\n",
    "\n",
    "Milestone 5: Final presentation and documentation (Week 11-12)\n",
    "\n",
    "**Data**\n",
    "\n",
    "Contract documents in various formats (PDF, DOCX, TXT). Predefined checklists and legal standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47dd2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported libraries\n",
    "import pdfplumber\n",
    "import docx\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3292b",
   "metadata": {},
   "source": [
    "# Einlesen von Vertragsdokumenten\n",
    "\n",
    "The contract checker tool that is going to be created in this project needs to be tested and trained based on some real world example contracts. Therefore Sectorelense provided us with an excel sheet containing a list of various providers of Saas solutions and links to their websites where sample contracts are available.\n",
    "\n",
    "These contract documents appear in various formats. Some of them in HTML, some in PDF, some in DOCX and some in the format of JSON.\n",
    "\n",
    "To automate the collection of contracts our first approach was to try to build an automated scraping tool for each file format.\n",
    "\n",
    "## Scraping HTML\n",
    "We Started by creating a scraping tool for HTML websites. After a short time we realised that this woulden´t be as easy as expected, since all the websites appear in different formats which leads to different scraping properties for every website.\n",
    "\n",
    "However we proceeded and tried to build a seperate scraping function for all the provided websites that seemed to be impactfull to us.\n",
    "\n",
    "The following code shows scraping functions for different kind of websites. In the end you can find a chooser function, that chooses which scraping functtion to use exactly based on the link provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c06f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Scraper für Standard-HTML-Verträge\n",
    "def scrape_html_standard(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        main_content = soup.find(\"div\", class_=\"single-content\") or soup\n",
    "        raw_text = main_content.get_text(separator=\" \", strip=True)\n",
    "        full_text = re.sub(r'\\s+', ' ', raw_text)\n",
    "\n",
    "        start_patterns = [r\"§\\s?\\d+\", r\"1\\.\\s+[^\\n\\.]+\"]\n",
    "        for pattern in start_patterns:\n",
    "            match = re.search(pattern, full_text)\n",
    "            if match:\n",
    "                full_text = full_text[match.start():]\n",
    "                break\n",
    "\n",
    "        end_markers = [\n",
    "            \"Die eingetragene Marke MOCO\", \"Stand 12/2024\", \"Ort, Datum\",\n",
    "            \"Unterschrift\", \"Impressum\", \"©\", \"Nachtrag Australische spezifische Begriffe\"\n",
    "        ]\n",
    "        cutoff = int(len(full_text) * 0.7)\n",
    "        positions = {m: full_text.find(m) for m in end_markers if full_text.find(m) > cutoff}\n",
    "        if positions:\n",
    "            full_text = full_text[:min(positions.values())]\n",
    "\n",
    "        return full_text.strip()\n",
    "\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 2. Scraper für CommonPaper-Verträge\n",
    "def scrape_html_commonpaper(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"entry-content\")\n",
    "        if not content:\n",
    "            print(f\"⚠️ CommonPaper: Kein Hauptbereich gefunden – {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        def walk_list(ol, prefix=\"\"):\n",
    "            items = ol.find_all(\"li\", recursive=False)\n",
    "            for idx, li in enumerate(items, 1):\n",
    "                number = f\"{prefix}.{idx}\" if prefix else str(idx)\n",
    "                li_copy = BeautifulSoup(str(li), \"html.parser\")\n",
    "                for sublist in li_copy.find_all(\"ol\"):\n",
    "                    sublist.decompose()\n",
    "                text = li_copy.get_text(separator=\" \", strip=True)\n",
    "                result.append(f\"{number}. {text}\")\n",
    "\n",
    "                sub_ol = li.find(\"ol\")\n",
    "                if sub_ol:\n",
    "                    walk_list(sub_ol, number)\n",
    "\n",
    "        top_ol = content.find(\"ol\")\n",
    "        if top_ol:\n",
    "            walk_list(top_ol)\n",
    "        else:\n",
    "            print(\"⚠️ Keine <ol> gefunden!\")\n",
    "\n",
    "        return \"\\n\".join(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen CommonPaper: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 3. Scraper für Fakturia-Verträge\n",
    "def scrape_html_fakturia(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"entry-content-wrapper\")\n",
    "        if not content:\n",
    "            print(\"⚠️ Fakturia: Kein Hauptbereich gefunden.\")\n",
    "            return \"\"\n",
    "\n",
    "        result = []\n",
    "        section = \"\"\n",
    "\n",
    "        for elem in content.find_all([\"h2\", \"p\"]):\n",
    "            text = re.sub(r'\\s+', ' ', elem.get_text(separator=\" \", strip=True))\n",
    "\n",
    "            if elem.name == \"h2\":\n",
    "                if section:\n",
    "                    result.append(section.strip())\n",
    "                section = text + \"\\n\"\n",
    "            elif elem.name == \"p\":\n",
    "                if re.match(r'^\\d+\\.\\d+', text):\n",
    "                    section += text + \" \"\n",
    "                else:\n",
    "                    section += text + \"\\n\"\n",
    "\n",
    "        if section:\n",
    "            result.append(section.strip())\n",
    "\n",
    "        for marker in [\"Copyright OSB Alliance e.V.\", \"gemäß CC BY\", \"Version 1/2015\"]:\n",
    "            if marker in result[-1]:\n",
    "                result[-1] = result[-1].split(marker)[0].strip()\n",
    "                break\n",
    "\n",
    "        return \"\\n\\n\".join(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen Fakturia: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 4. Scraper für Mitratech-Verträge\n",
    "def scrape_html_mitratech(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"form\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        main = soup.find(\"main\") or soup\n",
    "        found = False\n",
    "        blocks = []\n",
    "\n",
    "        for el in main.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"ol\", \"ul\"]):\n",
    "            text = el.get_text(separator=\" \", strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if not found and text.startswith(\"1. Allgemeines\"):\n",
    "                found = True\n",
    "                blocks.append(text)\n",
    "                continue\n",
    "\n",
    "            if found and el.name in [\"h1\", \"h2\", \"h3\"] and \"Begriffsbestimmungen\" in text:\n",
    "                break\n",
    "\n",
    "            if found:\n",
    "                blocks.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(blocks).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen Mitratech: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Automatische Auswahl je nach URL\n",
    "def scrape_contract_auto(url):\n",
    "    url_lc = url.lower()\n",
    "    if \"commonpaper.com\" in url_lc:\n",
    "        return scrape_html_commonpaper(url)\n",
    "    elif \"fakturia.de\" in url_lc:\n",
    "        return scrape_html_fakturia(url)\n",
    "    elif \"mitratech.com\" in url_lc or \"alyne.com\" in url_lc:\n",
    "        return scrape_html_mitratech(url)\n",
    "    else:\n",
    "        return scrape_html_standard(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850526f4",
   "metadata": {},
   "source": [
    "# Reading in PDF, DOCX and JSON\n",
    "\n",
    "Since we realised that all the files are delivered in different formats and therefore trying to automate the reading process won´t be really sucsesfull, since you have to write a new function for every document we stopped that approach. If we would continue like this we would have to write a seperate function for each document, considering the slight differences each document comes with.\n",
    "\n",
    "Since this would consume a lot of time and is not very efficient as prooven by the HTML example we decided to simply copy all the relevant DOCX, PDF and JSON files into TXT files manually. This is because it is way easier for us to read in txt files that are all in the same format.\n",
    "\n",
    "This project is about NLP and not so much about building automated scraping tools. Therefore we think this apporach is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011d86e",
   "metadata": {},
   "source": [
    "**TXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a73cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion zum einlesen von .txt files\n",
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Einlesen der Datei: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc401cc",
   "metadata": {},
   "source": [
    "**Mapping Datei einlesen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497f3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "excel_path = Path(\"../data/input_mapping/Mappingliste_Verträge.xlsx\")\n",
    "df = pd.read_excel(excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b27c23",
   "metadata": {},
   "source": [
    "**Neue Spalte Content und Filetype in DF erzeugen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a742b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Content' not in df.columns:\n",
    "    df['Content'] = \"\"\n",
    "\n",
    "if 'FileType' not in df.columns:\n",
    "    df['FileType'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9734e",
   "metadata": {},
   "source": [
    "**TxT files und HTML links automatisiert in Data Frame einlesen und als pickle file speichern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c8fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basisordner für lokale Vertragsdateien\n",
    "base_path = Path(\"../data/verträge/verträge_txt\")\n",
    "\n",
    "# Iteration über die Mapping-Tabelle\n",
    "for idx, row in df.iterrows():\n",
    "    mapping_field = row['Mapping']\n",
    "    content = \"\"\n",
    "    file_type = \"\"\n",
    "\n",
    "    if pd.notna(mapping_field):\n",
    "        mappings = [m.strip() for m in mapping_field.split(';')]\n",
    "        texts = []\n",
    "\n",
    "        for i, mapping in enumerate(mappings):\n",
    "            if mapping.endswith('.txt'):\n",
    "                filename = Path(mapping).name  # nur Dateiname\n",
    "                filepath = base_path / filename\n",
    "                texts.append(read_txt_file(filepath))\n",
    "                if i == 0:\n",
    "                    file_type = \"TXT\"\n",
    "            else:\n",
    "                texts.append(scrape_contract_auto(mapping))\n",
    "                if i == 0:\n",
    "                    file_type = \"HTML\"\n",
    "\n",
    "        content = \"\\n\\n\".join(texts)\n",
    "\n",
    "    df.at[idx, 'Content'] = content\n",
    "    df.at[idx, 'FileType'] = file_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eccfda",
   "metadata": {},
   "source": [
    "**Englische Texte übersetzen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# src/ zum Python-Pfad hinzufügen (nur nötig, wenn nicht automatisch erkannt)\n",
    "sys.path.append(str(Path(\"..\") / \"src\"))\n",
    "# Funktion importieren\n",
    "\n",
    "from translate import translate_english_documents\n",
    "#Funktioniert aber limitiert in der nutzung\n",
    "#df = translate_english_documents(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a5101",
   "metadata": {},
   "source": [
    "**Fertige input files als pickle file speichern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8237cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ausgabeordner und -datei\n",
    "output_pickle_path = Path(\"../data/data_scraped_input.pkl\")\n",
    "# Ergebnisse speichern\n",
    "df.to_pickle(output_pickle_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
