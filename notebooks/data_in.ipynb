{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f62f80",
   "metadata": {},
   "source": [
    "# NLP Project Sectorlense Vertragschecker\n",
    "\n",
    "**Projectdescription**\n",
    "\n",
    "Reviewing software contracts is often a complex and error-prone task, particularly when\n",
    "assessing standardized requirements and identifying potential risks. Manual contract review\n",
    "can be time-consuming, leading to inconsistencies and oversight. To address this challenge,\n",
    "the project aims to develop an LLM-based contract checker that automates the review\n",
    "process. By leveraging predefined checklists and legal standards, the system will\n",
    "systematically analyze contracts, ensuring that required clauses are present while also\n",
    "detecting critical or unusual formulations. This will streamline contract evaluation and\n",
    "facilitate structured risk assessment, reducing both time and effort for legal professionals\n",
    "and businesses.\n",
    "\n",
    "The contract checker will incorporate three primary functionalities. A standard compliance\n",
    "check will verify whether contracts include the necessary clauses and if they adhere to\n",
    "established legal and business standards. Assessment based on standardized criteria will\n",
    "evaluate key contractual aspects to ensure completeness and compliance. Risk identification\n",
    "will highlight non-standard, ambiguous, or high-risk clauses, enabling users to assess their\n",
    "appropriateness compared to standard contract terms. Additionally, an optional risk\n",
    "detection feature could be introduced to flag further potential risks that may not be explicitly\n",
    "covered in the predefined checklist.\n",
    "\n",
    "The final deliverable will be a web application that enables users to upload contract\n",
    "documents and receive an automated structured review including insights on compliance\n",
    "and risk factors. This application will provide detailed feedback, highlight critical sections,\n",
    "and suggest improvements, making contract review more efficient and reliable.\n",
    "Development will build upon an existing prototype that includes both a frontend and basic\n",
    "functionality, allowing for enhancements in accuracy, usability, and scalability.\n",
    "\n",
    "**Meilensteine**:\n",
    "\n",
    "Milestone 1: Understanding existing prototype and defining key requirements (Week 1-2)\n",
    "\n",
    "Milestone 2: Developing/improving NLP-based contract analysis model (Week 3-6)\n",
    "\n",
    "Milestone 3: Integration into the web application (Week 7-8)\n",
    "\n",
    "Milestone 4: Testing and evaluation with real-world contracts (Week 9-10)\n",
    "\n",
    "Milestone 5: Final presentation and documentation (Week 11-12)\n",
    "\n",
    "**Data**\n",
    "\n",
    "Contract documents in various formats (PDF, DOCX, TXT). Predefined checklists and legal standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47dd2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported libraries\n",
    "import pdfplumber\n",
    "import docx\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3292b",
   "metadata": {},
   "source": [
    "# Einlesen von Vertragsdokumenten\n",
    "\n",
    "The contract checker tool that is going to be created in this project needs to be tested and trained based on some real world example contracts. Therefore Sectorelense provided us with an excel sheet containing a list of various providers of Saas solutions and links to their websites where sample contracts are available.\n",
    "\n",
    "These contract documents appear in various formats. Some of them in HTML, some in PDF, some in DOCX and some in the format of JSON.\n",
    "\n",
    "To automate the collection of contracts our first approach was to try to build an automated scraping tool for each file format.\n",
    "\n",
    "## Scraping HTML\n",
    "We Started by creating a scraping tool for HTML websites. After a short time we realised that this woulden¬¥t be as easy as expected, since all the websites appear in different formats which leads to different scraping properties for every website.\n",
    "\n",
    "However we proceeded and tried to build a seperate scraping function for all the provided websites that seemed to be impactfull to us.\n",
    "\n",
    "The following code shows scraping functions for different kind of websites. In the end you can find a chooser function, that chooses which scraping functtion to use exactly based on the link provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c06f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Scraper f√ºr Standard-HTML-Vertr√§ge\n",
    "def scrape_html_standard(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        main_content = soup.find(\"div\", class_=\"single-content\") or soup\n",
    "        raw_text = main_content.get_text(separator=\" \", strip=True)\n",
    "        full_text = re.sub(r'\\s+', ' ', raw_text)\n",
    "\n",
    "        start_patterns = [r\"¬ß\\s?\\d+\", r\"1\\.\\s+[^\\n\\.]+\"]\n",
    "        for pattern in start_patterns:\n",
    "            match = re.search(pattern, full_text)\n",
    "            if match:\n",
    "                full_text = full_text[match.start():]\n",
    "                break\n",
    "\n",
    "        end_markers = [\n",
    "            \"Die eingetragene Marke MOCO\", \"Stand 12/2024\", \"Ort, Datum\",\n",
    "            \"Unterschrift\", \"Impressum\", \"¬©\", \"Nachtrag Australische spezifische Begriffe\"\n",
    "        ]\n",
    "        cutoff = int(len(full_text) * 0.7)\n",
    "        positions = {m: full_text.find(m) for m in end_markers if full_text.find(m) > cutoff}\n",
    "        if positions:\n",
    "            full_text = full_text[:min(positions.values())]\n",
    "\n",
    "        return full_text.strip()\n",
    "\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 2. Scraper f√ºr CommonPaper-Vertr√§ge\n",
    "def scrape_html_commonpaper(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"entry-content\")\n",
    "        if not content:\n",
    "            print(f\"‚ö†Ô∏è CommonPaper: Kein Hauptbereich gefunden ‚Äì {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        def walk_list(ol, prefix=\"\"):\n",
    "            items = ol.find_all(\"li\", recursive=False)\n",
    "            for idx, li in enumerate(items, 1):\n",
    "                number = f\"{prefix}.{idx}\" if prefix else str(idx)\n",
    "                li_copy = BeautifulSoup(str(li), \"html.parser\")\n",
    "                for sublist in li_copy.find_all(\"ol\"):\n",
    "                    sublist.decompose()\n",
    "                text = li_copy.get_text(separator=\" \", strip=True)\n",
    "                result.append(f\"{number}. {text}\")\n",
    "\n",
    "                sub_ol = li.find(\"ol\")\n",
    "                if sub_ol:\n",
    "                    walk_list(sub_ol, number)\n",
    "\n",
    "        top_ol = content.find(\"ol\")\n",
    "        if top_ol:\n",
    "            walk_list(top_ol)\n",
    "            print(f\"üèÅ CommonPaper: {len(result)} Punkte extrahiert.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Keine <ol> gefunden!\")\n",
    "\n",
    "        return \"\\n\".join(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen CommonPaper: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 3. Scraper f√ºr Fakturia-Vertr√§ge\n",
    "def scrape_html_fakturia(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"entry-content-wrapper\")\n",
    "        if not content:\n",
    "            print(\"‚ö†Ô∏è Fakturia: Kein Hauptbereich gefunden.\")\n",
    "            return \"\"\n",
    "\n",
    "        result = []\n",
    "        section = \"\"\n",
    "\n",
    "        for elem in content.find_all([\"h2\", \"p\"]):\n",
    "            text = re.sub(r'\\s+', ' ', elem.get_text(separator=\" \", strip=True))\n",
    "\n",
    "            if elem.name == \"h2\":\n",
    "                if section:\n",
    "                    result.append(section.strip())\n",
    "                section = text + \"\\n\"\n",
    "            elif elem.name == \"p\":\n",
    "                if re.match(r'^\\d+\\.\\d+', text):\n",
    "                    section += text + \" \"\n",
    "                else:\n",
    "                    section += text + \"\\n\"\n",
    "\n",
    "        if section:\n",
    "            result.append(section.strip())\n",
    "\n",
    "        for marker in [\"Copyright OSB Alliance e.V.\", \"gem√§√ü CC BY\", \"Version 1/2015\"]:\n",
    "            if marker in result[-1]:\n",
    "                result[-1] = result[-1].split(marker)[0].strip()\n",
    "                break\n",
    "\n",
    "        print(f\"üèÅ Fakturia: {len(result)} Abschnitte extrahiert.\")\n",
    "        return \"\\n\\n\".join(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen Fakturia: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# 4. Scraper f√ºr Mitratech-Vertr√§ge\n",
    "def scrape_html_mitratech(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"form\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        main = soup.find(\"main\") or soup\n",
    "        found = False\n",
    "        blocks = []\n",
    "\n",
    "        for el in main.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"ol\", \"ul\"]):\n",
    "            text = el.get_text(separator=\" \", strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if not found and text.startswith(\"1. Allgemeines\"):\n",
    "                found = True\n",
    "                blocks.append(text)\n",
    "                continue\n",
    "\n",
    "            if found and el.name in [\"h1\", \"h2\", \"h3\"] and \"Begriffsbestimmungen\" in text:\n",
    "                break\n",
    "\n",
    "            if found:\n",
    "                blocks.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(blocks).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scrapen Mitratech: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Automatische Auswahl je nach URL\n",
    "def scrape_contract_auto(url):\n",
    "    url_lc = url.lower()\n",
    "\n",
    "    if \"commonpaper.com\" in url_lc:\n",
    "        print(\"üåê CommonPaper erkannt ‚Äì verwende scrape_html_commonpaper()\")\n",
    "        return scrape_html_commonpaper(url)\n",
    "    elif \"fakturia.de\" in url_lc:\n",
    "        print(\"üåê Fakturia erkannt ‚Äì verwende scrape_html_fakturia()\")\n",
    "        return scrape_html_fakturia(url)\n",
    "    elif \"mitratech.com\" in url_lc or \"alyne.com\" in url_lc:\n",
    "        print(\"üåê Mitratech erkannt ‚Äì verwende scrape_html_mitratech()\")\n",
    "        return scrape_html_mitratech(url)\n",
    "    else:\n",
    "        print(\"üåê Standardvertrag erkannt ‚Äì verwende scrape_html_standard()\")\n",
    "        return scrape_html_standard(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850526f4",
   "metadata": {},
   "source": [
    "# Reading in PDF, DOCX and JSON\n",
    "\n",
    "Since we realised that all the files are delivered in different formats and therefore trying to automate the reading process won¬¥t be really sucsesfull, since you have to write a new function for every document we stopped that approach. If we would continue like this we would have to write a seperate function for each document, considering the slight differences each document comes with.\n",
    "\n",
    "Since this would consume a lot of time and is not very efficient as prooven by the HTML example we decided to simply copy all the relevant DOCX, PDF and JSON files into TXT files manually. This is because it is way easier for us to read in txt files that are all in the same format.\n",
    "\n",
    "This project is about NLP and not so much about building automated scraping tools. Therefore we think this apporach is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011d86e",
   "metadata": {},
   "source": [
    "**TXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a73cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion zum einlesen von .txt files\n",
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Einlesen der Datei: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc401cc",
   "metadata": {},
   "source": [
    "**Mapping Datei einlesen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497f3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "excel_path = Path(\"../data/input_mapping/Mappingliste_Vertr√§ge.xlsx\")\n",
    "df = pd.read_excel(excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b27c23",
   "metadata": {},
   "source": [
    "**Neue Spalte Content und Filetype in DF erzeugen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a742b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Content' not in df.columns:\n",
    "    df['Content'] = \"\"\n",
    "\n",
    "if 'FileType' not in df.columns:\n",
    "    df['FileType'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9734e",
   "metadata": {},
   "source": [
    "**TxT files und HTML links automatisiert in Data Frame einlesen und als pickle file speichern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c8fede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Standardvertrag erkannt ‚Äì verwende scrape_html_standard()\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Templates_3H_Solutions_AG_18-06_SaaS-Cloudsoftware_Vertrag.txt'\n",
      "üåê CommonPaper erkannt ‚Äì verwende scrape_html_commonpaper()\n",
      "üèÅ CommonPaper: 120 Punkte extrahiert.\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\SaaS_SAP_Service_Level_Agreement.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Saas_SAP_General_Terms.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\SaaS_SAP_Support_Scheudle.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Saas_Oracle_Cloud_Services_Vertrag.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Saas_Oracle_Data_Processing_Agreement.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Saas_Oracle_Subscription_Services_Agreement.txt'\n",
      "üåê Fakturia erkannt ‚Äì verwende scrape_html_fakturia()\n",
      "üèÅ Fakturia: 15 Abschnitte extrahiert.\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Hauptgruppe_Draeger.txt'\n",
      "üåê Standardvertrag erkannt ‚Äì verwende scrape_html_standard()\n",
      "üåê Standardvertrag erkannt ‚Äì verwende scrape_html_standard()\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Hauptgruppe_TA Triumph-Adler.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Hauptgruppe_acadon_AG_AGB.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Hauptgruppe_Alasco_GmbH_AGB.txt'\n",
      "üåê Mitratech erkannt ‚Äì verwende scrape_html_mitratech()\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\Hauptgruppe_Aptean_AGB.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\CSB-Systeme_AGB.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\GFOS_SAAS_AGB.txt'\n",
      "Fehler beim Einlesen der Datei: [Errno 2] No such file or directory: 'data\\\\vertr√§ge\\\\vertr√§ge_txt\\\\OSBA_Standard-Vertragsbedingungen_SaaS_Referenzrahmen.txt'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     df.at[idx, \u001b[33m'\u001b[39m\u001b[33mFileType\u001b[39m\u001b[33m'\u001b[39m] = file_type\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Ergebnisse speichern\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_pickle_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilst\\Documents\\GitHub_Repos\\NLP_Contract_Checker\\NLP_Contract_Checker\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilst\\Documents\\GitHub_Repos\\NLP_Contract_Checker\\NLP_Contract_Checker\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3165\u001b[39m, in \u001b[36mNDFrame.to_pickle\u001b[39m\u001b[34m(self, path, compression, protocol, storage_options)\u001b[39m\n\u001b[32m   3115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[33;03mPickle (serialize) object to file.\u001b[39;00m\n\u001b[32m   3117\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3161\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m   3162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_pickle\n\u001b[32m-> \u001b[39m\u001b[32m3165\u001b[39m \u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilst\\Documents\\GitHub_Repos\\NLP_Contract_Checker\\NLP_Contract_Checker\\.venv\\Lib\\site-packages\\pandas\\io\\pickle.py:103\u001b[39m, in \u001b[36mto_pickle\u001b[39m\u001b[34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m protocol < \u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     protocol = pickle.HIGHEST_PROTOCOL\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[32m    111\u001b[39m     pickle.dump(obj, handles.handle, protocol=protocol)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilst\\Documents\\GitHub_Repos\\NLP_Contract_Checker\\NLP_Contract_Checker\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilst\\Documents\\GitHub_Repos\\NLP_Contract_Checker\\NLP_Contract_Checker\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'data'"
     ]
    }
   ],
   "source": [
    "# Basisordner f√ºr lokale Vertragsdateien\n",
    "base_path = Path(\"data/vertr√§ge/vertr√§ge_txt\")\n",
    "\n",
    "# Ausgabeordner und -datei\n",
    "output_pickle_path = Path(\"data/data_scraped_input.pkl\")\n",
    "\n",
    "# Iteration √ºber die Mapping-Tabelle\n",
    "for idx, row in df.iterrows():\n",
    "    mapping_field = row['Mapping']\n",
    "    content = \"\"\n",
    "    file_type = \"\"\n",
    "\n",
    "    if pd.notna(mapping_field):\n",
    "        mappings = [m.strip() for m in mapping_field.split(';')]\n",
    "        texts = []\n",
    "\n",
    "        for i, mapping in enumerate(mappings):\n",
    "            if mapping.endswith('.txt'):\n",
    "                filename = Path(mapping).name  # nur Dateiname\n",
    "                filepath = base_path / filename\n",
    "                texts.append(read_txt_file(filepath))\n",
    "                if i == 0:\n",
    "                    file_type = \"TXT\"\n",
    "            else:\n",
    "                texts.append(scrape_contract_auto(mapping))\n",
    "                if i == 0:\n",
    "                    file_type = \"HTML\"\n",
    "\n",
    "        content = \"\\n\\n\".join(texts)\n",
    "\n",
    "    df.at[idx, 'Content'] = content\n",
    "    df.at[idx, 'FileType'] = file_type\n",
    "\n",
    "# Ergebnisse speichern\n",
    "df.to_pickle(output_pickle_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
